8644 8645 8646
8645 8646 8647
8646 8647 8648
8647 8648 8649
------------------------------
199   129   140   Token      Linguistics
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
129   140   11083 Organization Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
200   140   141   Token      :
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
201   141   152   Token      shortpapers
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
202   152   153   Token      ,
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
204   154   159   Token      pages
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
206   160   162   Token      89
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
207   162   163   Token      –
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
208   163   165   Token      94
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
209   165   166   Token      ,
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
210   166   174   Token      Portland
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
166   174   7778  Lookup     Portland
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
166   174   10600 Location   Portland
211   174   175   Token      ,
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
213   176   182   Token      Oregon
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
176   182   7779  Lookup     Oregon
176   182   10601 Location   Oregon
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
214   182   183   Token      ,
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
216   184   188   Token      June
184   200   10602 Date       June 19-24, 2011
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
184   188   7780  Lookup     June
184   188   7781  Lookup     June
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
218   189   191   Token      19
184   200   10602 Date       June 19-24, 2011
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
219   191   192   Token      -
184   200   10602 Date       June 19-24, 2011
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
220   192   194   Token      24
184   200   10602 Date       June 19-24, 2011
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
221   194   195   Token      ,
184   200   10602 Date       June 19-24, 2011
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
223   196   200   Token      2011
184   200   10602 Date       June 19-24, 2011
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
196   200   7782  Lookup     2011
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
224   200   201   Token      .
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
200   201   8351  Split      .
53    201   8645  Sentence   Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011.
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
226   202   203   Token      c
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
202   250   8646  Sentence   c©2011 Association for Computational Linguistics
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
227   203   204   Token      ©
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
202   250   8646  Sentence   c©2011 Association for Computational Linguistics
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
228   204   208   Token      2011
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
204   208   10603 Date       2011
204   208   7783  Lookup     2011
202   250   8646  Sentence   c©2011 Association for Computational Linguistics
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
230   209   220   Token      Association
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
209   220   7784  Lookup     Association
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
202   250   8646  Sentence   c©2011 Association for Computational Linguistics
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
232   221   224   Token      for
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
202   250   8646  Sentence   c©2011 Association for Computational Linguistics
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
234   225   238   Token      Computational
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
225   238   10728 Unknown    Computational
202   250   8646  Sentence   c©2011 Association for Computational Linguistics
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
236   239   250   Token      Linguistics
53    250   37    p          Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
202   250   8646  Sentence   c©2011 Association for Computational Linguistics
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
239   250   11084 Organization Linguistics
238   251   255   Token      That
251   303   8647  Sentence   That’s What She Said: Double Entendre Identification
251   303   38    p          That’s What She Said: Double Entendre Identification
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
251   255   7785  Lookup     That
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
239   255   256   Token      ’
251   303   8647  Sentence   That’s What She Said: Double Entendre Identification
251   303   38    p          That’s What She Said: Double Entendre Identification
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
240   256   257   Token      s
251   303   8647  Sentence   That’s What She Said: Double Entendre Identification
251   303   38    p          That’s What She Said: Double Entendre Identification
53    3554  35    div        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
53    21794 34    body       Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94
0     21794 0     html       That's What She Said: Double Entendre Identification
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 89–94,Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
That’s What She Said: Double Entendre Identification
Chloe´ Kiddon and Yuriy BrunComputer Science & Engineering
University of WashingtonSeattle WA 98195-2350
{chloe,brun}@cs.washington.edu
Abstract
Humor identification is a hard natural lan-guage understanding problem. We identifya subproblem — the “that’s what she said”problem — with two distinguishing character-istics: (1) use of nouns that are euphemismsfor sexually explicit nouns and (2) structurecommon in the erotic domain. We addressthis problem in a classification approach thatincludes features that model those two char-acteristics. Experiments on web data demon-strate that our approach improves precision by12% over baseline techniques that use onlyword-based features.
1 Introduction
“That’s what she said” is a well-known family ofjokes, recently repopularized by the television show“The Office” (Daniels et al., 2005). The jokes con-sist of saying “that’s what she said” after someoneelse utters a statement in a non-sexual context thatcould also have been used in a sexual context. Forexample, if Aaron refers to his late-evening basket-ball practice, saying “I was trying all night, but I justcould not get it in!”, Betty could utter “that’s whatshe said”, completing the joke. While somewhat ju-venile, this joke presents an interesting natural lan-guage understanding problem.
A “that’s what she said” (TWSS) joke is a type ofdouble entendre. A double entendre, or adianoeta,is an expression that can be understood in two differ-ent ways: an innocuous, straightforward way, giventhe context, and a risque´ way that indirectly alludesto a different, indecent context. To our knowledge,
related research has not studied the task of identify-ing double entendres in text or speech. The task iscomplex and would require both deep semantic andcultural understanding to recognize the vast array ofdouble entendres. We focus on a subtask of doubleentendre identification: TWSS recognition. We saya sentence is a TWSS if it is funny to follow thatsentence with “that’s what she said”.
We frame the problem of TWSS recognition asa type of metaphor identification. A metaphor isa figure of speech that creates an analogical map-ping between two conceptual domains so that theterminology of one (source) domain can be used todescribe situations and objects in the other (target)domain. Usage of the source domain’s terminol-ogy in the source domain is literal and is nonliteralin the target domain. Metaphor identification sys-tems seek to differentiate between literal and nonlit-eral expressions. Some computational approaches tometaphor identification learn selectional preferencesof words in multiple domains to help identify nonlit-eral usage (Mason, 2004; Shutova, 2010). Other ap-proaches train support vector machine (SVM) mod-els on labeled training data to distinguish metaphoriclanguage from literal language (Pasanek and Scul-ley, 2008).
TWSSs also represent mappings between two do-mains: the innocuous source domain and an erotictarget domain. Therefore, we can apply methodsfrom metaphor identification to TWSS identifica-tion. In particular, we (1) compare the adjectivalselectional preferences of sexually explicit nouns tothose of other nouns to determine which nouns maybe euphemisms for sexually explicit nouns and (2)
89
examine the relationship between structures in theerotic domain and nonerotic contexts. We presenta novel approach — Double Entendre via NounTransfer (DEviaNT) — that applies metaphor iden-tification techniques to solving the double entendreproblem and evaluate it on the TWSS problem. DE-viaNT classifies individual sentences as either funnyif followed by “that’s what she said” or not, whichis a type of automatic humor recognition (Mihal-cea and Strapparava, 2005; Mihalcea and Pulman,2007).
We argue that in the TWSS domain, high preci-sion is important, while low recall may be tolerated.In experiments on nearly 21K sentences, we findthat DEviaNT has 12% higher precision than that ofbaseline classifiers that use n-gram TWSS models.
The rest of this paper is structured as follows:Section 2 will outline the characteristics of theTWSS problem that we leverage in our approach.Section 3 will describe the DEviaNT approach. Sec-tion 4 will evaluate DEviaNT on the TWSS problem.Finally, Section 5 will summarize our contributions.
2 The TWSS Problem
We observe two facts about the TWSS problem.First, sentences with nouns that are euphemisms forsexually explicit nouns are more likely to be TWSSs.For example, containing the noun “banana” makesa sentence more likely to be a TWSS than contain-ing the noun “door”. Second, TWSSs share com-mon structure with sentences in the erotic domain.For example, a sentence of the form “[subject] stuck[object] in” or “[subject] could eat [object] all day”is more likely to be a TWSS than not. Thus, wehypothesize that machine learning with euphemism-and structure-based features is a promising approachto solving the TWSS problem. Accordingly, apartfrom a few basic features that define a TWSS joke(e.g., short sentence), all of our approach’s lexicalfeatures model a metaphorical mapping to objectsand structures in the erotic domain.
Part of TWSS identification is recognizing thatthe source context in which the potential TWSS isuttered is not in an erotic one. If it is, then the map-ping to the erotic domain is the identity and the state-ment is not a TWSS. In this paper, we assume all testinstances are from nonerotic domains and leave the
classification of erotic and nonerotic contexts to fu-ture work.
There are two interesting and important aspectsof the TWSS problem that make solving it difficult.First, many domains in which a TWSS classifiercould be applied value high precision significantlymore than high recall. For example, in a social set-ting, the cost of saying “that’s what she said” inap-propriately is high, whereas the cost of not sayingit when it might have been appropriate is negligible.For another example, in automated public tagging oftwitter and facebook data, false positives are consid-ered spam and violate usage policies, whereas falsenegatives go unnoticed. Second, the overwhelm-ing majority of everyday sentences are not TWSSs,making achieving high precision even more difficult.In this paper, we strive specifically to achieve highprecision but are willing to sacrifice recall.
3 The DEviaNT Approach
The TWSS problem has two identifying character-istics: (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain. Our approach to solving theTWSS problem is centered around an SVM modelthat uses features designed to model those charac-teristics. We call our approach Double Entendre viaNoun Transfer, or the DEviaNT approach.
We will use features that build on corpus statisticscomputed for known erotic words, and their lexicalcontexts, as described in the rest of this section.
3.1 Data and word classes
Let SN be an open set of sexually explicit nouns. Wemanually approximated SN with a set of 76 nounsthat are predominantly used in sexual contexts. Weclustered the nouns into 9 categories based on whichsexual object, body part, or participant they identify.Let SN− ⊂ SN be the set of sexually explicit nounsthat are likely targets for euphemism. We did notconsider euphemisms for people since they rarely, ifever, are used in TWSS jokes. In our approximation,∣∣SN−∣∣ = 61. Let BP be an open set of body-partnouns. Our approximation contains 98 body parts.
DEviaNT uses two corpora. The erotica corpusconsists of 1.5M sentences from the erotica section
90
of textfiles.com/sex/EROTICA. We removedheaders, footers, URLs, and unparseable text. TheBrown corpus (Francis and Kucera, 1979) is 57Ksentences that represent standard (nonerotic) litera-ture. We tagged the erotica corpus with the StanfordParser (Toutanova and Manning, 2000; Toutanovaet al., 2003); the Brown corpus is already tagged.To make the corpora more generic, we replaced allnumbers with the CD tag, all proper nouns with theNNP tag, all nouns ∈ SN with an SN tag, and allnouns 6∈ BP with the NN tag. We ignored determin-ers and punctuation.
3.2 Word- and phrase-level analysisWe define three functions to measure how closelyrelated a noun, an adjective, and a verb phrase are tothe erotica domain.
1. The noun sexiness function NS(n) is a real-valued measure of the maximum similarity a nounn /∈ SN has to each of the nouns ∈ SN−. For eachnoun, let the adjective count vector be the vector ofthe absolute frequencies of each adjective that mod-ifies the noun in the union of the erotica and theBrown corpora. We define NS(n) to be the maxi-mum cosine similarity, over each noun ∈ SN−, usingterm frequency-inverse document frequency (tf-idf)weights of the nouns’ adjective count vectors. Fornouns that occurred fewer that 200 times, occurredfewer than 50 times with adjectives, or were asso-ciated with 3 times as many adjectives that neveroccurred with nouns in SN than adjectives that did,NS(n) = 10−7 (smaller than all recorded similari-ties). Example nouns with high NS are “rod” and“meat”.
2. The adjective sexiness function AS(a) is areal-valued measure of how likely an adjective a isto modify a noun ∈ SN. We define AS(a) to be therelative frequency of a in sentences in the eroticacorpus that contain at least one noun ∈ SN. Exam-ple adjectives with high AS are “hot” and “wet”.
3. The verb sexiness function VS(v) is a real-valued measure of how much more likely a verbphrase v is to appear in an erotic context than anonerotic one. Let SE be the set of sentences in theerotica corpus that contain nouns ∈ SN. Let SB bethe set of all sentences in the Brown corpus. Givena sentence s containing a verb v, the verb phrase vis the contiguous substring of the sentence that con-
tains v and is bordered on each side by the closestnoun or one of the set of pronouns {I, you, it, me}.(If neither a noun nor none of the pronouns occur ona side of the verb, v itself is an endpoint of v.)
To define VS(v), we approximate the probabilitiesof v appearing in an erotic and a nonerotic contextwith counts in SE and SB, respectively. We normal-ize the counts in SB such that P(s∈ SE) = P(s∈ SB).Let VS(v) be the probability that (v ∈ s) =⇒ (s isin an erotic context). Then,
VS(v) = P(s ∈ SE |v ∈ s)=
P(v ∈ s|s ∈ SE)P(s ∈ SE)P(v ∈ s) .
Intuitively, the verb sexiness is a measure of howlikely the action described in a sentence could be anaction (via some metaphoric mapping) to an actionin an erotic context.
3.3 Features
DEviaNT uses the following features to identify po-tential mappings of a sentence s into the erotic do-main, organized into two categories: NOUN EU-PHEMISMS and STRUCTURAL ELEMENTS.NOUN EUPHEMISMS:• (boolean) does s contain a noun ∈ SN?,• (boolean) does s contain a noun ∈ BP?,• (boolean) does s contain a noun n such thatNS(n) = 10−7,• (real) average NS(n), for all nouns n ∈ s such
that n /∈ SN∪BP,
STRUCTURAL ELEMENTS:• (boolean) does s contain a verb that never oc-
curs in SE?,• (boolean) does s contain a verb phrase that
never occurs in SE?,• (real) average VS(v) over all verb phrases v∈ s,• (real) average AS(a) over all adjectives a ∈ s,• (boolean) does s contain an adjective a such
that a never occurs in a sentence s ∈ SE ∪ SBwith a noun ∈ SN.
DEviaNT also uses the following features to iden-tify the BASIC STRUCTURE of a TWSS:• (int) number of non-punctuation tokens,• (int) number of punctuation tokens,
91
• ({0, 1, 2+}) for each pronoun and each part-of-speech tag, number of times it occurs in s,• ({noun, proper noun, each of a selected group
of pronouns that can be used as subjects (e.g.,“she”, “it”), other pronoun}) the subject of s.(We approximate the subject with the first nounor pronoun.)
3.4 Learning algorithm
DEviaNT uses an SVM classifier from the WEKAmachine learning package (Hall et al., 2009) withthe features from Section 3.3. In our prototype im-plementation, DEviaNT uses the default parametersettings and has the option to fit logistic regressioncurves to the outputs to allow for precision-recallanalysis. To minimize false positives, while toler-ating false negatives, DEviaNT employs the Meta-Cost metaclassifier (Domingos, 1999), which usesbagging to reclassify the training data to producea single cost-sensitive classifier. DEviaNT sets thecost of a false positive to be 100 times that of a falsenegative.
4 Evaluation
The goal of our evaluation is somewhat unusual.DEviaNT explores a particular approach to solvingthe TWSS problem: recognizing euphemistic andstructural relationships between the source domainand an erotic domain. As such, DEviaNT is at a dis-advantage to many potential solutions because DE-viaNT does not aggressively explore features spe-cific to TWSSs (e.g., DEviaNT does not use a lexicaln-gram model of the TWSS training data). Thus, thegoal of our evaluation is not to outperform the base-lines in all aspects, but rather to show that by usingonly euphemism-based and structure-based features,DEviaNT can compete with the baselines, particu-larly where it matters most, delivering high precisionand few false positives.
4.1 Datasets
Our goals for DEviaNT’s training data were to(1) include a wide range of negative samples todistinguish TWSSs from arbitrary sentences while(2) keeping negative and positive samples similarenough in language to tackle difficult cases. DE-
viaNT’s positive training data are 2001 quoted sen-tences from twssstories.com (TS), a website ofuser-submitted TWSS jokes. DEviaNT’s negativetraining data are 2001 sentences from three sources(667 each): textsfromlastnight.com (TFLN), aset of user-submitted, typically-racy text messages;fmylife.com/intimacy (FML), a set of short (1–2 sentence) user-submitted stories about their lovelives; and wikiquote.org (WQ), a set of quotationsfrom famous American speakers and films. We didnot carefully examine these sources for noise, butgiven that TWSSs are rare, we assumed these dataare sufficiently negative. For testing, we used 262other TS and 20,700 other TFLN, FML, and WQsentences (all the data from these sources that wereavailable at the time of the experiments). We cleanedthe data by splitting it into individual sentences, cap-italizing the first letter of each sentence, tagging itwith the Stanford Parser (Toutanova and Manning,2000; Toutanova et al., 2003), and fixing several tag-ger errors (e.g., changing the tag of “i” from the for-eign word tag FW to the correct pronoun tag PRP).
4.2 Baselines
Our experiments compare DEviaNT to seven otherclassifiers: (1) a Naı¨ve Bayes classifier on unigramfeatures, (2) an SVM model trained on unigram fea-tures, (3) an SVM model trained on unigram andbigram features, (4–6) MetaCost (Domingos, 1999)(see Section 3.4) versions of (1–3), and (7) a versionof DEviaNT that uses just the BASIC STRUCTUREfeatures (as a feature ablation study). The SVMmodels use the same parameters and kernel functionas DEviaNT.
The state-of-the-practice approach to TWSS iden-tification is a naı¨ve Bayes model trained on a un-igram model of instances of twitter tweets, sometagged with #twss (VandenBos, 2011). While thiswas the only existing classifier we were able to find,this was not a rigorously approached solution to theproblem. In particular, its training data were noisy,partially untaggable, and multilingual. Thus, wereimplemented this approach more rigorously as oneof our baselines.
For completeness, we tested whether adding un-igram features to DEviaNT improved its perfor-mance but found that it did not.
92
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
P re c
i s io n
Recall
DEviaNTBasic Structure
Unigram SVM w/ MetaCostUnigram SVM w/o MetaCost
Bigram SVM w/ MetaCostBigram SVM w/o MetaCost
Naive Bayes w/ MetaCostNaive Bayes w/o MetaCost
Figure 1: The precision-recall curves for DEviaNT andbaseline classifiers on TS, TFLN, FML, and WQ.
4.3 Results
Figure 1 shows the precision-recall curves for DE-viaNT and the other seven classifiers. DEviaNT andBasic Structure achieve the highest precisions. Thebest competitor — Unigram SVM w/o MetaCost —has the maximum precision of 59.2%. In contrast,DEviaNT’s precision is over 71.4%. Note that theaddition of bigram features yields no improvementin (and can hurt) both precision and recall.
To qualitatively evaluate DEviaNT, we comparedthose sentences that DEviaNT, Basic Structure, andUnigram SVM w/o MetaCost are most sure areTWSSs. DEviaNT returned 28 such sentences (alltied for most likely to be a TWSS), 20 of whichare true positives. However, 2 of the 8 false pos-itives are in fact TWSSs (despite coming from thenegative testing data): “Yes give me all the creamand he’s gone.” and “Yeah but his hole really smellssometimes.” Basic Structure was most sure about 16sentences, 11 of which are true positives. Of these,7 were also in DEviaNT’s most-sure set. However,DEviaNT was also able to identify TWSSs that dealwith noun euphemisms (e.g., “Don’t you think thesebuns are a little too big for this meat?”), whereas Ba-sic Structure could not. In contrast, Unigram SVMw/o MetaCost is most sure about 130 sentences, 77of which are true positives. Note that while DE-
viaNT has a much lower recall than Unigram SVMw/o MetaCost, it accomplishes our goal of deliver-ing high-precision, while tolerating low recall.
Note that the DEviaNT’s precision appears low inlarge because the testing data is predominantly neg-ative. If DEviaNT classified a randomly selected,balanced subset of the test data, DEviaNT’s preci-sion would be 0.995.
5 Contributions
We formally defined the TWSS problem, a sub-problem of the double entendre problem. We thenidentified two characteristics of the TWSS prob-lem — (1) TWSSs are likely to contain nouns thatare euphemisms for sexually explicit nouns and (2)TWSSs share common structure with sentences inthe erotic domain — that we used to constructDEviaNT, an approach for TWSS classification.DEviaNT identifies euphemism and erotic-domainstructure without relying heavily on structural fea-tures specific to TWSSs. DEviaNT delivers sig-nificantly higher precision than classifiers that usen-gram TWSS models. Our experiments indicatethat euphemism- and erotic-domain-structure fea-tures contribute to improving the precision of TWSSidentification.
While significant future work in improving DE-viaNT remains, we have identified two character-istics important to the TWSS problem and demon-strated that an approach based on these character-istics has promise. The technique of metaphoricalmapping may be generalized to identify other typesof double entendres and other forms of humor.
Acknowledgments
The authors wish to thank Tony Fader and MarkYatskar for their insights and help with data, Bran-don Lucia for his part in coming up with the nameDEviaNT, and Luke Zettlemoyer for helpful com-ments. This material is based upon work supportedby the National Science Foundation Graduate Re-search Fellowship under Grant #DGE-0718124 andunder Grant #0937060 to the Computing ResearchAssociation for the CIFellows Project.
93
References
Greg Daniels, Ricky Gervais, and Stephen Mer-chant. 2005. The Office. Television series, theNational Broadcasting Company (NBC).
Pedro Domingos. 1999. MetaCost: A generalmethod for making classifiers cost-sensitive. InProceedings of the 5th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 155–164. San Diego, CA,USA.
W. Nelson Francis and Henry Kucera. 1979. A Stan-dard Corpus of Present-Day Edited American En-glish. Department of Linguistics, Brown Univer-sity.
Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009. The WEKA data mining software: An up-date. SIGKDD Explorations, 11(1).
Zachary J. Mason. 2004. CorMet: A computational,corpus-based conventional metaphor extractionsystem. Computational Linguistics, 30(1):23–44.
Rada Mihalcea and Stephen Pulman. 2007. Char-acterizing humour: An exploration of features inhumorous texts. In Proceedings of the 8th Con-ference on Intelligent Text Processing and Com-putational Linguistics (CICLing07). Mexico City,Mexico.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-ing computers laugh: Investigations in auto-matic humor recognition. In Human Language
Technology Conference / Conference on Empir-ical Methods in Natural Language Processing(HLT/EMNLP05). Vancouver, BC, Canada.
Bradley M. Pasanek and D. Sculley. 2008. Miningmillions of metaphors. Literary and LinguisticComputing, 23(3).
Ekaterina Shutova. 2010. Automatic metaphor inter-pretation as a paraphrasing task. In Proceedingsof Human Language Technologies: The 11th An-nual Conference of the North American Chapterof the Association for Computational Linguistics(HLT10), pages 1029–1037. Los Angeles, CA,USA.
Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency net-work. In Proceedings of Human Language Tech-nologies: The Annual Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics (HLT03), pages 252–259. Ed-monton, AB, Canada.
Kristina Toutanova and Christopher Manning. 2000.Enriching the knowledge sources used in a maxi-mum entropy part-of-speech tagger. In Joint SIG-DAT Conference on Empirical Methods in NLPand Very Large Corpora (EMNLP/VLC00), pages63–71. Hong Kong, China.
Ben VandenBos. 2011. Pre-trained “that’s what shesaid” bayes classifier. http://rubygems.org/gems/twss.
94

